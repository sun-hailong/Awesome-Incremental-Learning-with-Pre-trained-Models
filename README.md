# Awesome Incremental Learning with Pre-trained Models[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
    <a href=""><img src="https://img.shields.io/badge/AILpapers-v1.0-orange"></a>
    <a href=""><img
src="https://img.shields.io/github/stars/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Model?style=social"></a>
	<a href=""><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fsun-hailong%2FAwesome-Incremental-Learning-with-Pre-trained-Model&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"></a>
	<a href=""><img src="https://img.shields.io/github/last-commit/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Models"></a>
</p>

## ü§ó Contributing
### üöÄFeel free to [contact me]() or add [pull request](https://github.com/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Model/pulls) if you findüëÄ any interesting paper is missing.

<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

üìãMarkdown format:
```markdown
- Paper Name. (**Conference Year**) [[paper](link)] [[code](link)]
```

## üõ†Ô∏èToolbox
- PILOT: A Pre-Trained Model-Based Continual Learning Toolbox (**arXiv23**)[[paper](https://arxiv.org/abs/2309.07117)] [[code](https://github.com/sun-hailong/LAMDA-PILOT)]

## üìùSurvey
- Continual Learning with Pre-Trained Models: A Survey (**arXiv23**)[[paper](https://arxiv.org/abs/2401.16386)] [[code](https://github.com/sun-hailong/LAMDA-PILOT)]
- Deep Class-Incremental Learning: A Survey (**arXiv23**)[[paper](https://arxiv.org/abs/2302.03648)] [[code](https://github.com/zhoudw-zdw/CIL_Survey)]

## üìëPapers

### 2023
- Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need (**arXiv23**)[[paper](https://arxiv.org/abs/2303.07338)] [[code](https://github.com/zhoudw-zdw/RevisitingCIL)]
- PromptFusion: Decoupling Stability and Plasticity for Continual Learning (**arXiv23**)[[paper](https://arxiv.org/abs/2303.07223)]
- CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning (**CVPR23**)[[paper](https://arxiv.org/abs/2211.13218)] [[code](https://github.com/GT-RIPL/CODA-Prompt)]
- Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference (**AAAI23**)[[paper](https://arxiv.org/abs/2211.15969)] [[code](https://github.com/iamwangyabin/ESN)]
- PIVOT: Prompting for Video Continual Learning (**CVPR23**)[[paper](https://arxiv.org/abs/2212.04842)]
- DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning (**ICML23**)[[paper](https://arxiv.org/abs/2305.00380)]
- Learning Expressive Prompting With Residuals for Vision Transformers (**CVPR23**)[[paper](https://arxiv.org/abs/2303.15591)]
- Multimodal Parameter-Efficient Few-Shot Class Incremental Learning (**arXiv23**)[[paper](https://arxiv.org/abs/2303.04751)]
- Real-Time Evaluation in Online Continual Learning: A New Hope (**CVPR23 Highlight**)[[paper](https://arxiv.org/abs/2302.01047)] [[code](https://github.com/Yasir-Ghunaim/RealtimeOCL)]
- Remind of the Past: Incremental Learning with Analogical Prompts (**arXiv23**)[[paper](https://arxiv.org/abs/2303.13898)] [[code](https://github.com/ZhihengCV/A-Prompts)]
- On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code (**arXiv23**)[[paper](https://arxiv.org/abs/2305.04106)]
- AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AttriCLIP_A_Non-Incremental_Learner_for_Incremental_Knowledge_Learning_CVPR_2023_paper.pdf)]
- Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf)]
- Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf)]
- Continual Detection Transformer for Incremental Object Detection (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf)] [[code](https://github.com/yaoyao-liu/CL-DETR)]
- Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalb_Principles_of_Forgetting_in_Domain-Incremental_Semantic_Segmentation_in_Adverse_Weather_CVPR_2023_paper.pdf)]
- Computationally Budgeted Continual Learning: What Does Matter? (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf)] [[code](https://github.com/drimpossible/BudgetCL)]
- Unsupervised Continual Semantic Adaptation through Neural Rendering (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2023_paper.pdf)]
- ConStruct-VL: Data-Free Continual Structured VL Concepts Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf)] [[code](https://github.com/jamessealesmith/ConStruct-VL)]
- Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Brahma_A_Probabilistic_Framework_for_Lifelong_Test-Time_Adaptation_CVPR_2023_paper.pdf)] [[code](https://github.com/dhanajitb/petal)]
- Learning without Forgetting for Vision-Language Models (**arXiv23**)[[paper](https://arxiv.org/abs/2305.19270)]
- Image-Object-Specific Prompt Learning for Few-Shot Class-Incremental Learning (**arXiv23**)[[paper](https://arxiv.org/abs/2309.02833)]
- Introducing Language Guidance in Prompt-based Continual Learning (**ICCV23**)[[paper](https://arxiv.org/abs/2308.15827)]
- When Prompt-based Incremental Learning Does Not Meet Strong Pretraining (**ICCV23**)[[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_When_Prompt-based_Incremental_Learning_Does_Not_Meet_Strong_Pretraining_ICCV_2023_paper.pdf)] [[code](https://github.com/TOM-tym/APG)]
- Class Incremental Learning with Pre-trained Vision-Language Models (**arXiv23**)[[paper](https://arxiv.org/pdf/2310.20348.pdf)]
- Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality (**NeurIPS23**)[[paper](https://arxiv.org/abs/2310.07234)] [[code](https://github.com/thu-ml/HiDe-Prompt)]
- FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning (**NeurIPS23**)[[paper](https://arxiv.org/abs/2309.14062)] [[code](https://github.com/dipamgoswami/FeCAM)]
- RanPAC: Random Projections and Pre-trained Models for Continual Learning (**NeurIPS23**)[[paper](https://arxiv.org/abs/2307.02251)] [[code](https://github.com/RanPAC/RanPAC)]
- Continual Learners are Incremental Model Generalizers (**ICML23**)[[paper](http://arxiv.org/abs/2306.12026)]
- DDGR: Continual Learning with Deep Diffusion-based Generative Replay (**ICML23**)[[paper](https://openreview.net/pdf?id=RlqgQXZx6r)] [[code](https://github.com/xiaocangshengGR/DDGR)]
- Continual Vision-Language Representation Learning with Off-Diagonal Information (**ICML23**)[[paper](https://arxiv.org/abs/2305.07437)]
- Self-regulating Prompts: Foundational Model Adaptation without Forgetting (**ICCV23**)[[paper](https://arxiv.org/abs/2307.06948)] [[code](https://github.com/muzairkhattak/PromptSRC)]
- CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation (**ICCV23**)[[paper](https://browse.arxiv.org/pdf/2308.07146.pdf)] [[code](https://github.com/KevinLight831/CTP)]
- Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning (**ICCV23**)[[paper](https://arxiv.org/abs/2308.09303)] [[code](https://github.com/moonjunyyy/si-blurry)]
- First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning (**ICCV23**)[[paper](https://arxiv.org/abs/2303.13199)]
- Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models (**ICCV23**)[[paper](https://arxiv.org/pdf/2303.06628.pdf)] [[code](https://github.com/Thunderbeee/ZSCL)]
- A Unified Continual Learning Framework with General Parameter-Efficient Tuning (**ICCV23**)[[paper](https://arxiv.org/abs/2303.10070)] [[code](https://github.com/gqk/LAE)]
- SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model (**ICCV23**)[[paper](https://arxiv.org/pdf/2303.05118.pdf)] [[code](https://github.com/GengDavid/SLCA)]


### 2022
- Class-Incremental Learning with Strong Pre-trained Models (**CVPR22**)[[paper](https://arxiv.org/abs/2204.03634)] [[code](https://github.com/amazon-science/sp-cil)]
- Learning to Prompt for Continual Learning (**CVPR22**)[[paper](https://arxiv.org/abs/2112.08654)] [[code](https://github.com/google-research/l2p)]
- S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning (**NeurIPS22**)[[paper](https://arxiv.org/abs/2207.12819)] [[code]( https://github.com/iamwangyabin/S-Prompts)]
- Don't Stop Learning: Towards Continual Learning for the CLIP Model (**arXiv22**)[[paper](https://arxiv.org/abs/2207.09248)]
- DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning (**ECCV22**)[[paper](https://arxiv.org/abs/2204.04799)] [[code](https://github.com/google-research/l2p)]
- Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection (**COLING22**)[[paper](https://arxiv.org/abs/2204.07275)]
- Momentum-based Weight Interpolation of Strong Zero-Shot Models for Continual Learning (**NeurIPS22**)[[paper](https://arxiv.org/abs/2211.03186)]
- Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue (**ENNLP22**)[[paper](https://arxiv.org/abs/2210.07783)]
- CLIP model is an Efficient Continual Learner (**arXiv22**)[[paper](https://arxiv.org/abs/2210.03114)] [[code](https://github.com/vgthengane/Continual-CLIP)]
- Memory Efficient Continual Learning with Transformers (**NeurIPS22**)[[paper](https://arxiv.org/abs/2203.04640)]
- Continual Pre-Training Mitigates Forgetting in Language and Vision (**arXiv22**)[[paper](https://arxiv.org/abs/2205.09357)] [[code](https://github.com/AndreaCossu/continual-pretraining-nlp-vision)]
- Fine-tuned Language Models are Continual Learners (**arXiv22**)[[paper](https://arxiv.org/abs/2205.12393)] [[code](https://github.com/ThomasScialom/T0_continual_learning)]
- Continual Learning with Foundation Models: An Empirical Study of Latent Replay (**CoLLAs22**)[[paper](https://arxiv.org/abs/2205.00329)] [[code](https://github.com/oleksost/latent_CL)]
- Effect of scale on catastrophic forgetting in neural networks (**ICLR22**)[[paper](https://openreview.net/pdf?id=GhVS8_yPeEa)]
- Continual Training of Language Models for Few-Shot Learning (**arXiv22**)[[paper](https://arxiv.org/abs/2210.05549)] [[code](https://github.com/UIC-Liu-Lab/CPT)]
- CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks (**NeurIPS22**)[[paper](https://arxiv.org/abs/2206.09059)] [[code](https://github.com/GLAMOR-USC/CLiMB)]
- A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning (**arXiv22**)[[paper](https://arxiv.org/abs/2210.04428)] [[code](https://github.com/UIC-Liu-Lab/CPT)]
- ELLE: Efficient Lifelong Pre-training for Emerging Data (**ACL22**)[[paper](https://arxiv.org/abs/2203.06311)] [[code](https://github.com/thunlp/ELLE)]


### 2021
- An Empirical Investigation of the Role of Pre-training in Lifelong Learning (**arXiv21**)[[paper](https://arxiv.org/abs/2112.09153)] [[code](https://github.com/sanketvmehta/lifelong-learning-pretraining-and-sam)]
- Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning (**ENNLP21**)[[paper](https://arxiv.org/abs/2104.08808)] [[code](https://github.com/INK-USC/CLIF)]