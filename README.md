# Awesome Incremental Learning with Pre-trained Models[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
    <a href=""><img src="https://img.shields.io/badge/AILpapers-v1.0-orange"></a>
    <a href=""><img
src="https://img.shields.io/github/stars/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Model?style=social"></a>
	<a href=""><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fsun-hailong%2FAwesome-Incremental-Learning-with-Pre-trained-Model&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"></a>
	<a href=""><img src="https://img.shields.io/github/last-commit/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Models"></a>
</p>

## ü§ó Contributing
### üöÄFeel free to [contact me]() or add [pull request](https://github.com/sun-hailong/Awesome-Incremental-Learning-with-Pre-trained-Model/pulls) if you findüëÄ any interesting paper is missing.

<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

üìãMarkdown format:
```markdown
- Paper Name. (**Conference Year**) [[paper](link)] [[code](link)]
```

## üõ†Ô∏èToolbox
- PILOT: A Pre-Trained Model-Based Continual Learning Toolbox (**arXiv23**)[[paper](https://arxiv.org/abs/2309.07117)] [[code](https://github.com/sun-hailong/LAMDA-PILOT)]

## üìëPapers

### 2023
- Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need (**arXiv23**)[[paper](https://arxiv.org/abs/2303.07338)] [[code](https://github.com/zhoudw-zdw/RevisitingCIL)]
- PromptFusion: Decoupling Stability and Plasticity for Continual Learning (**arXiv23**)[[paper](https://arxiv.org/abs/2303.07223)]
- Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models (**arXiv23**)[[paper](https://arxiv.org/abs/2303.06628)] [[code](https://github.com/Thunderbeee/ZSCL)]
- A Unified Continual Learning Framework with General Parameter-Efficient Tuning (**arXiv23**)[[paper](https://arxiv.org/abs/2303.10070)] [[code](https://github.com/gqk/LAE)]
- SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model (**arXiv23**)[[paper](https://arxiv.org/abs/2303.05118)]
- PLOT: Prompt Learning with Optimal Transport for Vision-Language Models (**ICLR23**)[[paper](https://arxiv.org/abs/2210.01253)] [[code](https://github.com/CHENGY12/PLOT)]
- CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning (**CVPR23**)[[paper](https://arxiv.org/abs/2211.13218)] [[code](https://github.com/GT-RIPL/CODA-Prompt)]
- Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference (**AAAI23**)[[paper](https://arxiv.org/abs/2211.15969)] [[code](https://github.com/iamwangyabin/ESN)]
- PIVOT: Prompting for Video Continual Learning (**CVPR23**)[[paper](https://arxiv.org/abs/2212.04842)]
- Deep Class-Incremental Learning: A Survey (**arXiv23**)[[paper](https://arxiv.org/abs/2302.03648)] [[code](https://github.com/zhoudw-zdw/CIL_Survey)]
- DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning (**ICML23**)[[paper](https://arxiv.org/abs/2305.00380)]
- Learning Expressive Prompting With Residuals for Vision Transformers (**CVPR23**)[[paper](https://arxiv.org/abs/2303.15591)]
- Multimodal Parameter-Efficient Few-Shot Class Incremental Learning (**arXiv23**)[[paper](https://arxiv.org/abs/2303.04751)]
- Real-Time Evaluation in Online Continual Learning: A New Hope (**CVPR23 Highlight**)[[paper](https://arxiv.org/abs/2302.01047)] [[code](https://github.com/Yasir-Ghunaim/RealtimeOCL)]
- Remind of the Past: Incremental Learning with Analogical Prompts (**arXiv23**)[[paper](https://arxiv.org/abs/2303.13898)] [[code](https://github.com/ZhihengCV/A-Prompts)]
- On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code (**arXiv23**)[[paper](https://arxiv.org/abs/2305.04106)]
- AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AttriCLIP_A_Non-Incremental_Learner_for_Incremental_Knowledge_Learning_CVPR_2023_paper.pdf)]
- Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf)]
- Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf)]
- Continual Detection Transformer for Incremental Object Detection (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf)] [[code](https://github.com/yaoyao-liu/CL-DETR)]
- Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalb_Principles_of_Forgetting_in_Domain-Incremental_Semantic_Segmentation_in_Adverse_Weather_CVPR_2023_paper.pdf)]
- Computationally Budgeted Continual Learning: What Does Matter? (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf)] [[code](https://github.com/drimpossible/BudgetCL)]
- Unsupervised Continual Semantic Adaptation through Neural Rendering (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2023_paper.pdf)]
- ConStruct-VL: Data-Free Continual Structured VL Concepts Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf)] [[code](https://github.com/jamessealesmith/ConStruct-VL)]
- Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning (**CVPR23**)[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Brahma_A_Probabilistic_Framework_for_Lifelong_Test-Time_Adaptation_CVPR_2023_paper.pdf)] [[code](https://github.com/dhanajitb/petal)]
- Learning without Forgetting for Vision-Language Models (**arXiv23**)[[paper](https://arxiv.org/abs/2305.19270)]


### 2022
- Class-Incremental Learning with Strong Pre-trained Models (**CVPR22**)[[paper](https://arxiv.org/abs/2204.03634)] [[code](https://github.com/amazon-science/sp-cil)]
- Learning to Prompt for Continual Learning (**CVPR22**)[[paper](https://arxiv.org/abs/2112.08654)] [[code](https://github.com/google-research/l2p)]
- S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning (**NeurIPS22**)[[paper](https://arxiv.org/abs/2207.12819)] [[code]( https://github.com/iamwangyabin/S-Prompts)]
- Don't Stop Learning: Towards Continual Learning for the CLIP Model (**arXiv22**)[[paper](https://arxiv.org/abs/2207.09248)]
- DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning (**ECCV22**)[[paper](https://arxiv.org/abs/2204.04799)] [[code](https://github.com/google-research/l2p)]
- Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection (**COLING22**)[[paper](https://arxiv.org/abs/2204.07275)]
- Momentum-based Weight Interpolation of Strong Zero-Shot Models for Continual Learning (**NeurIPS22**)[[paper](https://arxiv.org/abs/2211.03186)]
- Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue (**ENNLP22**)[[paper](https://arxiv.org/abs/2210.07783)]
- CLIP model is an Efficient Continual Learner (**arXiv22**)[[paper](https://arxiv.org/abs/2210.03114)] [[code](https://github.com/vgthengane/Continual-CLIP)]
- Memory Efficient Continual Learning with Transformers (**NeurIPS22**)[[paper](https://arxiv.org/abs/2203.04640)]
- Continual Pre-Training Mitigates Forgetting in Language and Vision (**arXiv22**)[[paper](https://arxiv.org/abs/2205.09357)] [[code](https://github.com/AndreaCossu/continual-pretraining-nlp-vision)]
- Fine-tuned Language Models are Continual Learners (**arXiv22**)[[paper](https://arxiv.org/abs/2205.12393)] [[code](https://github.com/ThomasScialom/T0_continual_learning)]
- Continual Learning with Foundation Models: An Empirical Study of Latent Replay (**CoLLAs22**)[[paper](https://arxiv.org/abs/2205.00329)] [[code](https://github.com/oleksost/latent_CL)]
- Effect of scale on catastrophic forgetting in neural networks (**ICLR22**)[[paper](https://openreview.net/pdf?id=GhVS8_yPeEa)]
- Continual Training of Language Models for Few-Shot Learning (**arXiv22**)[[paper](https://arxiv.org/abs/2210.05549)] [[code](https://github.com/UIC-Liu-Lab/CPT)]
- CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks (**NeurIPS22**)[[paper](https://arxiv.org/abs/2206.09059)] [[code](https://github.com/GLAMOR-USC/CLiMB)]
- A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning (**arXiv22**)[[paper](https://arxiv.org/abs/2210.04428)] [[code](https://github.com/UIC-Liu-Lab/CPT)]


### 2021
- An Empirical Investigation of the Role of Pre-training in Lifelong Learning (**arXiv21**)[[paper](https://arxiv.org/abs/2112.09153)] [[code](https://github.com/sanketvmehta/lifelong-learning-pretraining-and-sam)]
- Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning (**ENNLP21**)[[paper](https://arxiv.org/abs/2104.08808)] [[code](https://github.com/INK-USC/CLIF)]